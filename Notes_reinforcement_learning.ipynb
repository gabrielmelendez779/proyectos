{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Notes_reinforcement_learning.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMdYh3i24Ymyo4UQjOrXFAv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabrielmelendez779/proyectos/blob/master/Notes_reinforcement_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2Po52BErluQ"
      },
      "source": [
        "## The Basic Idea of RL\r\n",
        "\r\n",
        "Let's suppose we are teaching a dog(agent) to cathc a ball. Instead of Teaching the dog explicitly to catch a ball, we just throw aball a every time  the dog catches the ball, we givethe dog a cookie (reward). If the dog fails, the we do not give it a cookie. Som the dog will understand that catching the ball caused to receive  a cookie and will attempt to repeat catching the ball. \" The dog will learn to catch the ball while aiming to maximize the cookies it can receive\"\r\n",
        "\r\n",
        "Then RL do not teach to an agent what to do, instead  we will to give a reward when he does the correct action, and a negative reward when he perform a incorrect action.\r\n",
        "\r\n",
        "In RL we want to train to an agent who is not live (ha ha) just virtualy. In the book they expose the example of train a robot to walk without hit a mountain, an the when the robot walk trough the propper path then he is awarded with \"ponts\" let say + 1.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0Ep4eLjwUUO"
      },
      "source": [
        "## The Algorithm\r\n",
        "\r\n",
        "1 .- The agent interacts whit the enviroment by performing an action, he begin with a random action.\r\n",
        "\r\n",
        "2 .- By performing 1 the agent move from one state to another\r\n",
        "\r\n",
        "3 .- The agent will receive a rewardbased on the action it performed.\r\n",
        "\r\n",
        "4 .- Based on 3 the aent will understand whether the action is good or bad\r\n",
        "\r\n",
        "5 .- If the action was good, and he receive a reward then the agent will prefer performing that action, else he will try with another action in search a positive reward.\r\n",
        "\r\n",
        "\"The model learns by maximizing the reward\" and that it's the main difference between supervised, unsupervising and reinforcement learning.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naGbAmyryzKl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}